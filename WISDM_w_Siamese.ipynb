{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumethR/Gait_Analysis_wSmartwatch/blob/model-nb/WISDM_w_Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Reo4LuYqvy8h",
        "outputId": "37a17530-fcaa-405e-9931-b2d3e3403bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "accelerometer_data_file_path = '/content/drive/MyDrive/Implementation/watch/accel'\n",
        "acc_files = os.listdir(accelerometer_data_file_path)\n",
        "print(acc_files[0]) #Note that this won't print the first file in the directory, but the list has all files\n",
        "\n",
        "gyroscope_data_file_path = '/content/drive/MyDrive/Implementation/watch/gyro'\n",
        "gyro_files = os.listdir(gyroscope_data_file_path) #Note that the list contains 52 files because the first one is a .DS_Store file\n",
        "print(gyro_files[0])"
      ],
      "metadata": {
        "id": "v6PNX0hoyZ1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464ef377-a80b-4386-9043-1ece675d2faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_1626_accel_watch.txt\n",
            "data_1629_gyro_watch.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map respective gyro file for the subject for the selected acc file\n",
        "wisdm_dataset = {}\n",
        "\n",
        "for acc_file in acc_files:\n",
        "  #get the subject_id\n",
        "  acc_file_name = acc_file.split(\"_\") #The second index will contain the subject_id\n",
        "  for gyro_file in gyro_files:\n",
        "    gyro_file_name = gyro_file.split(\"_\")\n",
        "    if acc_file_name[1] == gyro_file_name[1]:\n",
        "      wisdm_dataset[acc_file] = gyro_file\n",
        "      gyro_files.remove(gyro_file)\n",
        "\n",
        "print(wisdm_dataset)"
      ],
      "metadata": {
        "id": "urHmN6IgycbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88bc991b-26a4-423b-8fb4-381dc40c38f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data_1626_accel_watch.txt': 'data_1626_gyro_watch.txt', 'data_1631_accel_watch.txt': 'data_1631_gyro_watch.txt', 'data_1605_accel_watch.txt': 'data_1605_gyro_watch.txt', 'data_1608_accel_watch.txt': 'data_1608_gyro_watch.txt', 'data_1650_accel_watch.txt': 'data_1650_gyro_watch.txt', 'data_1600_accel_watch.txt': 'data_1600_gyro_watch.txt', 'data_1623_accel_watch.txt': 'data_1623_gyro_watch.txt', 'data_1612_accel_watch.txt': 'data_1612_gyro_watch.txt', 'data_1620_accel_watch.txt': 'data_1620_gyro_watch.txt', 'data_1637_accel_watch.txt': 'data_1637_gyro_watch.txt', 'data_1618_accel_watch.txt': 'data_1618_gyro_watch.txt', 'data_1633_accel_watch.txt': 'data_1633_gyro_watch.txt', 'data_1635_accel_watch.txt': 'data_1635_gyro_watch.txt', 'data_1611_accel_watch.txt': 'data_1611_gyro_watch.txt', 'data_1619_accel_watch.txt': 'data_1619_gyro_watch.txt', 'data_1601_accel_watch.txt': 'data_1601_gyro_watch.txt', 'data_1614_accel_watch.txt': 'data_1614_gyro_watch.txt', 'data_1616_accel_watch.txt': 'data_1616_gyro_watch.txt', '.DS_Store': '.DS_Store', 'data_1645_accel_watch.txt': 'data_1645_gyro_watch.txt', 'data_1606_accel_watch.txt': 'data_1606_gyro_watch.txt', 'data_1615_accel_watch.txt': 'data_1615_gyro_watch.txt', 'data_1638_accel_watch.txt': 'data_1638_gyro_watch.txt', 'data_1607_accel_watch.txt': 'data_1607_gyro_watch.txt', 'data_1636_accel_watch.txt': 'data_1636_gyro_watch.txt', 'data_1627_accel_watch.txt': 'data_1627_gyro_watch.txt', 'data_1625_accel_watch.txt': 'data_1625_gyro_watch.txt', 'data_1643_accel_watch.txt': 'data_1643_gyro_watch.txt', 'data_1640_accel_watch.txt': 'data_1640_gyro_watch.txt', 'data_1602_accel_watch.txt': 'data_1602_gyro_watch.txt', 'data_1644_accel_watch.txt': 'data_1644_gyro_watch.txt', 'data_1642_accel_watch.txt': 'data_1642_gyro_watch.txt', 'data_1629_accel_watch.txt': 'data_1629_gyro_watch.txt', 'data_1624_accel_watch.txt': 'data_1624_gyro_watch.txt', 'data_1610_accel_watch.txt': 'data_1610_gyro_watch.txt', 'data_1603_accel_watch.txt': 'data_1603_gyro_watch.txt', 'data_1621_accel_watch.txt': 'data_1621_gyro_watch.txt', 'data_1646_accel_watch.txt': 'data_1646_gyro_watch.txt', 'data_1628_accel_watch.txt': 'data_1628_gyro_watch.txt', 'data_1604_accel_watch.txt': 'data_1604_gyro_watch.txt', 'data_1634_accel_watch.txt': 'data_1634_gyro_watch.txt', 'data_1613_accel_watch.txt': 'data_1613_gyro_watch.txt', 'data_1647_accel_watch.txt': 'data_1647_gyro_watch.txt', 'data_1609_accel_watch.txt': 'data_1609_gyro_watch.txt', 'data_1630_accel_watch.txt': 'data_1630_gyro_watch.txt', 'data_1639_accel_watch.txt': 'data_1639_gyro_watch.txt', 'data_1648_accel_watch.txt': 'data_1648_gyro_watch.txt', 'data_1617_accel_watch.txt': 'data_1617_gyro_watch.txt', 'data_1632_accel_watch.txt': 'data_1632_gyro_watch.txt', 'data_1649_accel_watch.txt': 'data_1649_gyro_watch.txt', 'data_1641_accel_watch.txt': 'data_1641_gyro_watch.txt', 'data_1622_accel_watch.txt': 'data_1622_gyro_watch.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the same set of steps for a second file\n",
        "def format_and_segment_data(acc_data, gyro_data):\n",
        "    def round_to_10_seconds(timestamp):\n",
        "        return timestamp - (timestamp % 10**10)  # 10 seconds in nanoseconds\n",
        "\n",
        "    def format_data(raw_data):\n",
        "        formatted_data = []\n",
        "        for entry in raw_data:\n",
        "            split_entry = entry[:-1].split(',')\n",
        "            split_entry[2] = int(split_entry[2])\n",
        "            for index in range(3, len(split_entry)):\n",
        "                split_entry[index] = float(split_entry[index])\n",
        "            formatted_data.append(split_entry)\n",
        "        return formatted_data\n",
        "\n",
        "    def segment_data(formatted_data):\n",
        "        segmented_data = {}\n",
        "        for line in formatted_data:\n",
        "            if line[1] == \"A\": #We are only using the walking\n",
        "                rounded_time = round_to_10_seconds(line[2])\n",
        "                segment_start = rounded_time\n",
        "                segment_end = rounded_time + 10**10  # 10 seconds in nanoseconds\n",
        "\n",
        "                if segment_start not in segmented_data:\n",
        "                    segmented_data[segment_start] = []\n",
        "\n",
        "                #Sampling rate for the WISDM dataset is 20Hz. THis is make sure we have mathematically correct number of readings\n",
        "                if(len(segmented_data[segment_start]) < 200):\n",
        "                  segmented_data[segment_start].append(line)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Deleting first and last keys to avoid segments that don't contain a full 10 seconds of data\n",
        "        if segmented_data:\n",
        "            first_key = next(iter(segmented_data))\n",
        "            last_key = list(segmented_data.keys())[-1]\n",
        "            del segmented_data[first_key]\n",
        "            del segmented_data[last_key]\n",
        "\n",
        "        return segmented_data\n",
        "\n",
        "    acc_formatted_data = format_data(acc_data)\n",
        "    gyro_formatted_data = format_data(gyro_data)\n",
        "\n",
        "    acc_segmented = segment_data(acc_formatted_data)\n",
        "    gyro_segmented = segment_data(gyro_formatted_data)\n",
        "\n",
        "    # Check if dictionaries have the same keys\n",
        "    if set(acc_segmented.keys()) == set(gyro_segmented.keys()):\n",
        "      print(\"The dictionaries have the same keys.\")\n",
        "    else:\n",
        "      print(\"The dictionaries have different keys.\")\n",
        "      # If the dictionaries don't have the same keys, use a set to make refs\n",
        "      # matched_timestamps = set(set1_acc_segmented_data.keys()).intersection(set1_gyro_segmented_data.keys())\n",
        "      # print(len(matched_timestamps))\n",
        "\n",
        "    return acc_segmented, gyro_segmented\n",
        "\n",
        "# Assuming you have acc_data and gyro_data ready\n",
        "set1_acc_file_path = '/content/drive/MyDrive/Implementation/watch/accel/data_1626_accel_watch.txt'\n",
        "set1_gyro_file_path = '/content/drive/MyDrive/Implementation/watch/gyro/data_1626_gyro_watch.txt'\n",
        "\n",
        "set2_acc_file_path = '/content/drive/MyDrive/Implementation/watch/accel/data_1627_accel_watch.txt'\n",
        "set2_gyro_file_path = '/content/drive/MyDrive/Implementation/watch/gyro/data_1627_gyro_watch.txt'\n",
        "\n",
        "def read_files(acc_file_path, gyro_file_path):\n",
        "  #reads the files from the drive and split them into seperate lines\n",
        "  with open(acc_file_path, 'r') as file:\n",
        "    acc_content = file.read()\n",
        "    acc_data = acc_content.splitlines()\n",
        "    print(acc_data[0])\n",
        "\n",
        "  with open(gyro_file_path, 'r') as file:\n",
        "    gyro_content = file.read()\n",
        "    gyro_data = gyro_content.splitlines()\n",
        "    print(gyro_data[0])\n",
        "\n",
        "  return format_and_segment_data(acc_data, gyro_data)\n",
        "\n",
        "set1_acc_segmented_data, set1_gyro_segmented_data = read_files(set1_acc_file_path, set1_gyro_file_path)\n",
        "set2_acc_segmented_data, set2_gyro_segmented_data = read_files(set2_acc_file_path, set2_gyro_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "307xA0uWy5--",
        "outputId": "ddc36be7-41f6-42d5-cd43-450cfe19ddd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1626,A,372824809784090,7.230639,-0.8752304,-0.18510172;\n",
            "1626,A,372824859284090,0.23433474,0.58682114,2.0321581;\n",
            "The dictionaries have the same keys.\n",
            "1627,A,216836851128086,2.726846,0.23732524,-4.664354;\n",
            "1627,A,216836900628086,1.0744395,2.2197602,0.5899488;\n",
            "The dictionaries have the same keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the values from both acc and gyro into one timestamp\n",
        "def combine_data(acc_data, gyro_data):\n",
        "  # Current format {(Timestamp): [[sensorX, sensorY, sensorZ], [sensorX, sensorY, sensorZ] ... 198 ]}\n",
        "  # Both dictionaries has the same key (Timestamp).\n",
        "  combined_map = {}\n",
        "  for key, value in acc_data.items():\n",
        "    acc_values = acc_data[key]\n",
        "    gyro_values = gyro_data[key]\n",
        "\n",
        "    #create an empty array for each timestep. and then add the combined data to that array\n",
        "    combined_map[key] = []\n",
        "\n",
        "    for x in acc_values:\n",
        "      #check if there is a corresponding timestamp in the gyro_values for each acc_value\n",
        "      for y in gyro_values:\n",
        "        x[2] = y[2]\n",
        "        # if the timestamps are equal create an array with both the values\n",
        "        array_all = [x[3], x[4], x[5], y[3], y[4], y[5]] #include x[0] if subject_id is needed in the future\n",
        "        combined_map[key].append(array_all)\n",
        "        break\n",
        "\n",
        "  return combined_map\n",
        "\n",
        "set1_combined = combine_data(set1_acc_segmented_data, set1_gyro_segmented_data)\n",
        "set2_combined = combine_data(set2_acc_segmented_data, set2_gyro_segmented_data)"
      ],
      "metadata": {
        "id": "XoRge8m9uzBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pairs and labels\n",
        "# {timestamp: [[[], [], []]],[]}\n",
        "# set1_combined, set2_combined\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def create_pairs():\n",
        "  pairs = []\n",
        "  labels = []\n",
        "  random_keys = set()\n",
        "\n",
        "  # Run a loop for 8 times\n",
        "  for i in range(0,9):\n",
        "    #select a random key from the set\n",
        "    random_pair = []\n",
        "\n",
        "    while True:\n",
        "      random_key = random.choice(list(set1_combined.keys()))\n",
        "      if random_key not in random_keys:\n",
        "        random_keys.add(random_key)\n",
        "        random_pair.append(set1_combined[random_key])\n",
        "        break\n",
        "\n",
        "    while True:\n",
        "      random_key = random.choice(list(set2_combined.keys()))\n",
        "      if random_key not in random_keys:\n",
        "        random_keys.add(random_key)\n",
        "        random_pair.append(set2_combined[random_key])\n",
        "        break\n",
        "\n",
        "    labels.append(0) #because the two sequences are from two different people\n",
        "    pairs.append(random_pair)\n",
        "\n",
        "  set1_keys = set(set1_combined.keys())\n",
        "  remaining_keys = set1_keys.difference(random_keys)\n",
        "\n",
        "  for i in range(len(pairs), len(set1_combined)):\n",
        "    true_pair = []\n",
        "    for i in remaining_keys:\n",
        "      if len(true_pair) < 2:\n",
        "        true_pair.append(set1_combined[i])\n",
        "      else:\n",
        "        pairs.append(true_pair)\n",
        "        labels.append(1)\n",
        "        break\n",
        "\n",
        "  print(len(pairs))\n",
        "  print(labels)\n",
        "  return pairs, labels\n",
        "\n",
        "pairs, labels = create_pairs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa54bjsZemGC",
        "outputId": "1b98201d-8f19-4726-dc2f-ab7fbd5b6cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle pairs and labels, so that the model does not lean any patterns\n",
        "# Combine pairs and labels\n",
        "combined_data = list(zip(pairs, labels))\n",
        "\n",
        "# Shuffle the combined data\n",
        "random.shuffle(combined_data)\n",
        "\n",
        "# Unpack the shuffled data back into pairs and labels\n",
        "shuffled_pairs, shuffled_labels = zip(*combined_data)\n",
        "\n",
        "train_sequences, val_sequences, train_labels, val_labels = train_test_split(shuffled_pairs, shuffled_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Number of training data sets: \", len(train_sequences))\n",
        "print(\"Each set has an array of length: \", len(train_sequences[0]))\n",
        "print(\"One of those two elements in the array contain\", len(train_sequences[0][0]), \"data points (20haz polling rate, 10 seconds of data).\")\n",
        "print(\"Each data point has\", len(train_sequences[0][0][0]), \"elements. 3 accelerometer values and 3 gyroscope values\")\n",
        "\n",
        "train_pairs = np.array(train_sequences)\n",
        "val_pairs = np.array(val_sequences)\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "print(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_UW91uIPxRG",
        "outputId": "c5b60c28-ec0b-4b02-ddfe-dd2b68eaf59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data sets:  13\n",
            "Each set has an array of length:  2\n",
            "One of those two elements in the array contain 200 data points (20haz polling rate, 10 seconds of data).\n",
            "Each data point has 6 elements. 3 accelerometer values and 3 gyroscope values\n",
            "[0 1 0 0 1 0 0 1 0 1 1 1 0]\n",
            "(13,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating pairs of data for the Simaese network. Ideally the data should look something like this.\n",
        "\n",
        "Each subject has 17 10-second data instances. The last element in the 2D-Array indicates weather the data is from the same person or not.\n",
        "\n",
        "1 = Same person\n",
        "0 = Different people\n",
        "\n",
        "> [Timestamp] : [[accX, accY, accZ], [gyroX, gyroY, gyroZ], [1]]\n",
        "\n",
        "> [Timestamp] : [[accX, accY, accZ], [gyroX, gyroY, gyroZ], [0]]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IHLeCIqTs8zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the Siamese model for sequence data\n",
        "def create_siamese_network(input_shape):\n",
        "    # Define the base LSTM network (twin network)\n",
        "    base_network = models.Sequential([\n",
        "        layers.LSTM(64, input_shape=input_shape, return_sequences=True),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(64, activation='relu')\n",
        "    ])\n",
        "\n",
        "    # Inputs for two sequences of accelerometer and gyroscope data\n",
        "    input_a = tf.keras.Input(shape=input_shape)\n",
        "    input_b = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Process each sequence through the twin network\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    # Calculate absolute difference between the processed outputs\n",
        "    distance = tf.keras.layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([processed_a, processed_b])\n",
        "\n",
        "    # Output layer for similarity prediction\n",
        "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "    # Siamese model\n",
        "    siamese_model = tf.keras.Model(inputs=[input_a, input_b], outputs=output)\n",
        "    return siamese_model\n",
        "\n",
        "# Define input shape (adjust based on your sequence length and sensor data)\n",
        "input_shape = (200, 6)  # Replace with your input shape\n",
        "siamese_network = create_siamese_network(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "siamese_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "siamese_network.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60G5lQYyfzC",
        "outputId": "fc0568a0-3644-4e39-835c-42d310a021c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 200, 6)]             0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 200, 6)]             0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 64)                   125248    ['input_1[0][0]',             \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 64)                   0         ['sequential[0][0]',          \n",
            "                                                                     'sequential[1][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    65        ['lambda[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125313 (489.50 KB)\n",
            "Trainable params: 125313 (489.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # Training pairs\n",
        "    train_labels,  # Training labels\n",
        "    epochs=10,  # Number of epochs\n",
        "    batch_size=32,  # Batch size\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_labels)  # Validation data\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXgT5Ioe_Ckc",
        "outputId": "3cffa6e2-7bb2-4b0d-843d-3cbd32f5b7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 13s 13s/step - loss: 0.6903 - accuracy: 0.6154 - val_loss: 0.7024 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 0.6667 - accuracy: 0.6154 - val_loss: 0.6900 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 0.6553 - accuracy: 0.7692 - val_loss: 0.6800 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 0.6322 - accuracy: 0.7692 - val_loss: 0.6716 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.6054 - accuracy: 0.7692 - val_loss: 0.6613 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 0.5804 - accuracy: 0.8462 - val_loss: 0.6557 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 0.5475 - accuracy: 0.8462 - val_loss: 0.6563 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.5112 - accuracy: 0.8462 - val_loss: 0.6702 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 0.4739 - accuracy: 0.8462 - val_loss: 0.6693 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 0.4252 - accuracy: 0.9231 - val_loss: 0.6783 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0b4436cd90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a HDF5 file (including architecture, weights, and training configuration)\n",
        "siamese_network.save('siamese_model.tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z9LZEGoDFbU",
        "outputId": "57e18d9f-a5c3-4b28-deeb-d64304b85906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}