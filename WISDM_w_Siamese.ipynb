{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RumethR/Gait_Analysis_wSmartwatch/blob/model-nb/WISDM_w_Siamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Reo4LuYqvy8h",
        "outputId": "6dbfd2b2-a91f-4886-b8f3-2fea18c89d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "accelerometer_data_file_path = '/content/drive/MyDrive/Implementation/watch/accel'\n",
        "acc_files = os.listdir(accelerometer_data_file_path)\n",
        "print(acc_files[0]) #Note that this won't print the first file in the directory, but the list has all files\n",
        "\n",
        "gyroscope_data_file_path = '/content/drive/MyDrive/Implementation/watch/gyro'\n",
        "gyro_files = os.listdir(gyroscope_data_file_path) #Note that the list contains 52 files because the first one is a .DS_Store file\n",
        "print(gyro_files[0])"
      ],
      "metadata": {
        "id": "v6PNX0hoyZ1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfe720a-8b9b-4493-c390-665da17bcd20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_1626_accel_watch.txt\n",
            "data_1629_gyro_watch.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Map respective gyro file for the subject for the selected acc file\n",
        "wisdm_dataset = {}\n",
        "\n",
        "for acc_file in acc_files:\n",
        "  #get the subject_id\n",
        "  acc_file_name = acc_file.split(\"_\") #The second index will contain the subject_id\n",
        "  for gyro_file in gyro_files:\n",
        "    gyro_file_name = gyro_file.split(\"_\")\n",
        "    if acc_file_name[1] == gyro_file_name[1]:\n",
        "      wisdm_dataset[acc_file] = gyro_file\n",
        "      gyro_files.remove(gyro_file)\n",
        "\n",
        "print(wisdm_dataset)"
      ],
      "metadata": {
        "id": "urHmN6IgycbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ddd1c9-b1a8-49f4-ef68-30fe86404268"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data_1626_accel_watch.txt': 'data_1626_gyro_watch.txt', 'data_1631_accel_watch.txt': 'data_1631_gyro_watch.txt', 'data_1605_accel_watch.txt': 'data_1605_gyro_watch.txt', 'data_1608_accel_watch.txt': 'data_1608_gyro_watch.txt', 'data_1650_accel_watch.txt': 'data_1650_gyro_watch.txt', 'data_1600_accel_watch.txt': 'data_1600_gyro_watch.txt', 'data_1623_accel_watch.txt': 'data_1623_gyro_watch.txt', 'data_1612_accel_watch.txt': 'data_1612_gyro_watch.txt', 'data_1620_accel_watch.txt': 'data_1620_gyro_watch.txt', 'data_1637_accel_watch.txt': 'data_1637_gyro_watch.txt', 'data_1618_accel_watch.txt': 'data_1618_gyro_watch.txt', 'data_1633_accel_watch.txt': 'data_1633_gyro_watch.txt', 'data_1635_accel_watch.txt': 'data_1635_gyro_watch.txt', 'data_1611_accel_watch.txt': 'data_1611_gyro_watch.txt', 'data_1619_accel_watch.txt': 'data_1619_gyro_watch.txt', 'data_1601_accel_watch.txt': 'data_1601_gyro_watch.txt', 'data_1614_accel_watch.txt': 'data_1614_gyro_watch.txt', 'data_1616_accel_watch.txt': 'data_1616_gyro_watch.txt', '.DS_Store': '.DS_Store', 'data_1645_accel_watch.txt': 'data_1645_gyro_watch.txt', 'data_1606_accel_watch.txt': 'data_1606_gyro_watch.txt', 'data_1615_accel_watch.txt': 'data_1615_gyro_watch.txt', 'data_1638_accel_watch.txt': 'data_1638_gyro_watch.txt', 'data_1607_accel_watch.txt': 'data_1607_gyro_watch.txt', 'data_1636_accel_watch.txt': 'data_1636_gyro_watch.txt', 'data_1627_accel_watch.txt': 'data_1627_gyro_watch.txt', 'data_1625_accel_watch.txt': 'data_1625_gyro_watch.txt', 'data_1643_accel_watch.txt': 'data_1643_gyro_watch.txt', 'data_1640_accel_watch.txt': 'data_1640_gyro_watch.txt', 'data_1602_accel_watch.txt': 'data_1602_gyro_watch.txt', 'data_1644_accel_watch.txt': 'data_1644_gyro_watch.txt', 'data_1642_accel_watch.txt': 'data_1642_gyro_watch.txt', 'data_1629_accel_watch.txt': 'data_1629_gyro_watch.txt', 'data_1624_accel_watch.txt': 'data_1624_gyro_watch.txt', 'data_1610_accel_watch.txt': 'data_1610_gyro_watch.txt', 'data_1603_accel_watch.txt': 'data_1603_gyro_watch.txt', 'data_1621_accel_watch.txt': 'data_1621_gyro_watch.txt', 'data_1646_accel_watch.txt': 'data_1646_gyro_watch.txt', 'data_1628_accel_watch.txt': 'data_1628_gyro_watch.txt', 'data_1604_accel_watch.txt': 'data_1604_gyro_watch.txt', 'data_1634_accel_watch.txt': 'data_1634_gyro_watch.txt', 'data_1613_accel_watch.txt': 'data_1613_gyro_watch.txt', 'data_1647_accel_watch.txt': 'data_1647_gyro_watch.txt', 'data_1609_accel_watch.txt': 'data_1609_gyro_watch.txt', 'data_1630_accel_watch.txt': 'data_1630_gyro_watch.txt', 'data_1639_accel_watch.txt': 'data_1639_gyro_watch.txt', 'data_1648_accel_watch.txt': 'data_1648_gyro_watch.txt', 'data_1617_accel_watch.txt': 'data_1617_gyro_watch.txt', 'data_1632_accel_watch.txt': 'data_1632_gyro_watch.txt', 'data_1649_accel_watch.txt': 'data_1649_gyro_watch.txt', 'data_1641_accel_watch.txt': 'data_1641_gyro_watch.txt', 'data_1622_accel_watch.txt': 'data_1622_gyro_watch.txt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the same set of steps for a second file\n",
        "def format_and_segment_data(acc_data, gyro_data):\n",
        "    def round_to_10_seconds(timestamp):\n",
        "        return timestamp - (timestamp % 10**10)  # 10 seconds in nanoseconds\n",
        "\n",
        "    def format_data(raw_data):\n",
        "        formatted_data = []\n",
        "        for entry in raw_data:\n",
        "            split_entry = entry[:-1].split(',')\n",
        "            split_entry[2] = int(split_entry[2])\n",
        "            for index in range(3, len(split_entry)):\n",
        "                split_entry[index] = float(split_entry[index])\n",
        "            formatted_data.append(split_entry)\n",
        "        return formatted_data\n",
        "\n",
        "    def segment_data(formatted_data):\n",
        "        segmented_data = {}\n",
        "        for line in formatted_data:\n",
        "            if line[1] == \"A\": #We are only using the walking\n",
        "                rounded_time = round_to_10_seconds(line[2])\n",
        "                segment_start = rounded_time\n",
        "                segment_end = rounded_time + 10**10  # 10 seconds in nanoseconds\n",
        "\n",
        "                if segment_start not in segmented_data:\n",
        "                    segmented_data[segment_start] = []\n",
        "\n",
        "                #Sampling rate for the WISDM dataset is 20Hz. THis is make sure we have mathematically correct number of readings\n",
        "                if(len(segmented_data[segment_start]) < 200):\n",
        "                  segmented_data[segment_start].append(line)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Deleting first and last keys to avoid segments that don't contain a full 10 seconds of data\n",
        "        if segmented_data:\n",
        "            first_key = next(iter(segmented_data))\n",
        "            last_key = list(segmented_data.keys())[-1]\n",
        "            del segmented_data[first_key]\n",
        "            del segmented_data[last_key]\n",
        "\n",
        "        return segmented_data\n",
        "\n",
        "    acc_formatted_data = format_data(acc_data)\n",
        "    gyro_formatted_data = format_data(gyro_data)\n",
        "\n",
        "    acc_segmented = segment_data(acc_formatted_data)\n",
        "    gyro_segmented = segment_data(gyro_formatted_data)\n",
        "\n",
        "    # Check if dictionaries have the same keys\n",
        "    if set(acc_segmented.keys()) == set(gyro_segmented.keys()):\n",
        "      print(\"The dictionaries have the same keys.\")\n",
        "    else:\n",
        "      print(\"The dictionaries have different keys.\")\n",
        "      # If the dictionaries don't have the same keys, use a set to make refs\n",
        "      # matched_timestamps = set(set1_acc_segmented_data.keys()).intersection(set1_gyro_segmented_data.keys())\n",
        "      # print(len(matched_timestamps))\n",
        "\n",
        "    return acc_segmented, gyro_segmented\n",
        "\n",
        "# Assuming you have acc_data and gyro_data ready\n",
        "set1_acc_file_path = '/content/drive/MyDrive/Implementation/watch/accel/data_1626_accel_watch.txt'\n",
        "set1_gyro_file_path = '/content/drive/MyDrive/Implementation/watch/gyro/data_1626_gyro_watch.txt'\n",
        "\n",
        "set2_acc_file_path = '/content/drive/MyDrive/Implementation/watch/accel/data_1627_accel_watch.txt'\n",
        "set2_gyro_file_path = '/content/drive/MyDrive/Implementation/watch/gyro/data_1627_gyro_watch.txt'\n",
        "\n",
        "def read_files(acc_file_path, gyro_file_path):\n",
        "  #reads the files from the drive and split them into seperate lines\n",
        "  with open(acc_file_path, 'r') as file:\n",
        "    acc_content = file.read()\n",
        "    acc_data = acc_content.splitlines()\n",
        "    print(acc_data[0])\n",
        "\n",
        "  with open(gyro_file_path, 'r') as file:\n",
        "    gyro_content = file.read()\n",
        "    gyro_data = gyro_content.splitlines()\n",
        "    print(gyro_data[0])\n",
        "\n",
        "  return format_and_segment_data(acc_data, gyro_data)\n",
        "\n",
        "set1_acc_segmented_data, set1_gyro_segmented_data = read_files(set1_acc_file_path, set1_gyro_file_path)\n",
        "set2_acc_segmented_data, set2_gyro_segmented_data = read_files(set2_acc_file_path, set2_gyro_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "307xA0uWy5--",
        "outputId": "af46f5be-016a-4e39-aa6e-bce4f2765756"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1626,A,372824809784090,7.230639,-0.8752304,-0.18510172;\n",
            "1626,A,372824859284090,0.23433474,0.58682114,2.0321581;\n",
            "The dictionaries have the same keys.\n",
            "1627,A,216836851128086,2.726846,0.23732524,-4.664354;\n",
            "1627,A,216836900628086,1.0744395,2.2197602,0.5899488;\n",
            "The dictionaries have the same keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the values from both acc and gyro into one timestamp\n",
        "def combine_data(acc_data, gyro_data):\n",
        "  # Current format {(Timestamp): [[sensorX, sensorY, sensorZ], [sensorX, sensorY, sensorZ] ... 198 ]}\n",
        "  # Both dictionaries has the same key (Timestamp).\n",
        "  combined_map = {}\n",
        "  for key, value in acc_data.items():\n",
        "    acc_values = acc_data[key]\n",
        "    gyro_values = gyro_data[key]\n",
        "\n",
        "    #create an empty array for each timestep. and then add the combined data to that array\n",
        "    combined_map[key] = []\n",
        "\n",
        "    for x in acc_values:\n",
        "      #check if there is a corresponding timestamp in the gyro_values for each acc_value\n",
        "      for y in gyro_values:\n",
        "        x[2] = y[2]\n",
        "        # if the timestamps are equal create an array with both the values\n",
        "        array_all = [x[3], x[4], x[5], y[3], y[4], y[5]] #include x[0] if subject_id is needed in the future\n",
        "        combined_map[key].append(array_all)\n",
        "        break\n",
        "\n",
        "  return combined_map\n",
        "\n",
        "set1_combined = combine_data(set1_acc_segmented_data, set1_gyro_segmented_data)\n",
        "set2_combined = combine_data(set2_acc_segmented_data, set2_gyro_segmented_data)"
      ],
      "metadata": {
        "id": "XoRge8m9uzBE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pairs and labels\n",
        "# {timestamp: [[[], [], []]],[]}\n",
        "# set1_combined, set2_combined\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def create_pairs():\n",
        "  pairs = []\n",
        "  labels = []\n",
        "  random_keys = set()\n",
        "\n",
        "  # Run a loop for 8 times\n",
        "  for i in range(0,9):\n",
        "    #select a random key from the set\n",
        "    random_pair = []\n",
        "\n",
        "    while True:\n",
        "      random_key = random.choice(list(set1_combined.keys()))\n",
        "      if random_key not in random_keys:\n",
        "        random_keys.add(random_key)\n",
        "        random_pair.append(set1_combined[random_key])\n",
        "        break\n",
        "\n",
        "    while True:\n",
        "      random_key = random.choice(list(set2_combined.keys()))\n",
        "      if random_key not in random_keys:\n",
        "        random_keys.add(random_key)\n",
        "        random_pair.append(set2_combined[random_key])\n",
        "        break\n",
        "\n",
        "    labels.append(0) #because the two sequences are from two different people\n",
        "    pairs.append(random_pair)\n",
        "\n",
        "  set1_keys = set(set1_combined.keys())\n",
        "  remaining_keys = set1_keys.difference(random_keys)\n",
        "\n",
        "  for i in range(len(pairs), len(set1_combined)):\n",
        "    true_pair = []\n",
        "    for i in remaining_keys:\n",
        "      if len(true_pair) < 2:\n",
        "        true_pair.append(set1_combined[i])\n",
        "      else:\n",
        "        pairs.append(true_pair)\n",
        "        labels.append(1)\n",
        "        break\n",
        "\n",
        "  print(len(pairs))\n",
        "  print(labels)\n",
        "  return pairs, labels\n",
        "\n",
        "pairs, labels = create_pairs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa54bjsZemGC",
        "outputId": "e69a38d9-c0db-4973-97ff-24f89d283255"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle pairs and labels, so that the model does not lean any patterns\n",
        "# Combine pairs and labels\n",
        "combined_data = list(zip(pairs, labels))\n",
        "\n",
        "# Shuffle the combined data\n",
        "random.shuffle(combined_data)\n",
        "\n",
        "# Unpack the shuffled data back into pairs and labels\n",
        "shuffled_pairs, shuffled_labels = zip(*combined_data)\n",
        "\n",
        "train_sequences, val_sequences, train_labels, val_labels = train_test_split(shuffled_pairs, shuffled_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Number of training data sets: \", len(train_sequences))\n",
        "print(\"Each set has an array of length: \", len(train_sequences[0]))\n",
        "print(\"One of those two elements in the array contain\", len(train_sequences[0][0]), \"data points (20haz polling rate, 10 seconds of data).\")\n",
        "print(\"Each data point has\", len(train_sequences[0][0][0]), \"elements. 3 accelerometer values and 3 gyroscope values\")\n",
        "\n",
        "train_pairs = np.array(train_sequences)\n",
        "val_pairs = np.array(val_sequences)\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "print(train_labels)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_UW91uIPxRG",
        "outputId": "b7b222d2-1aea-4b72-f3ed-e9e2fb899070"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data sets:  13\n",
            "Each set has an array of length:  2\n",
            "One of those two elements in the array contain 200 data points (20haz polling rate, 10 seconds of data).\n",
            "Each data point has 6 elements. 3 accelerometer values and 3 gyroscope values\n",
            "[1 0 0 1 0 0 0 0 1 0 1 1 1]\n",
            "(13,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating pairs of data for the Simaese network. Ideally the data should look something like this.\n",
        "\n",
        "Each subject has 17 10-second data instances. The last element in the 2D-Array indicates weather the data is from the same person or not.\n",
        "\n",
        "1 = Same person\n",
        "0 = Different people\n",
        "\n",
        "> [Timestamp] : [[accX, accY, accZ], [gyroX, gyroY, gyroZ], [1]]\n",
        "\n",
        "> [Timestamp] : [[accX, accY, accZ], [gyroX, gyroY, gyroZ], [0]]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IHLeCIqTs8zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the Siamese model for sequence data\n",
        "def create_siamese_network(input_shape):\n",
        "    # Define the base LSTM network (twin network)\n",
        "    base_network = models.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(64, input_shape=input_shape, return_sequences=True)),\n",
        "        layers.LSTM(256),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(64, activation='relu')\n",
        "    ])\n",
        "\n",
        "    # Inputs for two sequences of accelerometer and gyroscope data\n",
        "    input_a = tf.keras.Input(shape=input_shape)\n",
        "    input_b = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Process each sequence through the twin network\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    # Calculate absolute difference between the processed outputs\n",
        "    distance = tf.keras.layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([processed_a, processed_b])\n",
        "\n",
        "    # Output layer for similarity prediction\n",
        "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "    # Siamese model\n",
        "    siamese_model = tf.keras.Model(inputs=[input_a, input_b], outputs=output)\n",
        "    return siamese_model\n",
        "\n",
        "# Define input shape (adjust based on your sequence length and sensor data)\n",
        "input_shape = (200, 6)  # Replace with your input shape\n",
        "siamese_network = create_siamese_network(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "siamese_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "siamese_network.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60G5lQYyfzC",
        "outputId": "331e6dc6-30ea-4ba8-99d7-a99356dadb20"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)       [(None, 200, 6)]             0         []                            \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)       [(None, 200, 6)]             0         []                            \n",
            "                                                                                                  \n",
            " sequential_9 (Sequential)   (None, 64)                   471744    ['input_11[0][0]',            \n",
            "                                                                     'input_12[0][0]']            \n",
            "                                                                                                  \n",
            " lambda_5 (Lambda)           (None, 64)                   0         ['sequential_9[0][0]',        \n",
            "                                                                     'sequential_9[1][0]']        \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 1)                    65        ['lambda_5[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 471809 (1.80 MB)\n",
            "Trainable params: 471809 (1.80 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # Training pairs\n",
        "    train_labels,  # Training labels\n",
        "    epochs=30,  # Number of epochs\n",
        "    batch_size=32,  # Batch size\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_labels)  # Validation data\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXgT5Ioe_Ckc",
        "outputId": "43b5bfb0-fecd-4a5a-b370-52e6ed1b1f5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1/1 [==============================] - 14s 14s/step - loss: 0.6732 - accuracy: 0.9231 - val_loss: 0.6067 - val_accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6409 - accuracy: 0.5385 - val_loss: 0.5557 - val_accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 1s 995ms/step - loss: 0.5938 - accuracy: 1.0000 - val_loss: 0.4819 - val_accuracy: 1.0000\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 1s 956ms/step - loss: 0.5393 - accuracy: 1.0000 - val_loss: 0.4181 - val_accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4798 - accuracy: 1.0000 - val_loss: 0.3447 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 1s 942ms/step - loss: 0.4043 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2684 - accuracy: 1.0000 - val_loss: 0.2120 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2605 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1415 - accuracy: 1.0000 - val_loss: 0.1298 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 1s 942ms/step - loss: 0.1306 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 1s 970ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 1s 948ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 1s 990ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 1s 983ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 1s 973ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 1s 954ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 1s 942ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 1s 949ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 1s 990ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 1s 931ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 1s 989ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 1s 941ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a4da1d0acb0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the CNN model\n",
        "def create_cnn_siamese_network(input_shape):\n",
        "    # Define the base LSTM network (twin network)\n",
        "    base_network = models.Sequential([\n",
        "        layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Flatten(),\n",
        "        #layers.Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5)\n",
        "    ])\n",
        "\n",
        "    # Inputs for two sequences of accelerometer and gyroscope data\n",
        "    input_a = tf.keras.Input(shape=input_shape)\n",
        "    input_b = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Process each sequence through the twin network\n",
        "    processed_a = base_network(input_a)\n",
        "    processed_b = base_network(input_b)\n",
        "\n",
        "    # Calculate absolute difference between the processed outputs\n",
        "    distance = tf.keras.layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([processed_a, processed_b])\n",
        "\n",
        "    # Output layer for similarity prediction\n",
        "    output = layers.Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "    # Siamese model\n",
        "    siamese_model = tf.keras.Model(inputs=[input_a, input_b], outputs=output)\n",
        "    return siamese_model\n",
        "\n",
        "# Define input shape (adjust based on your sequence length and sensor data)\n",
        "input_shape = (200, 6)  # Replace with your input shape\n",
        "cnn_siamese_network = create_cnn_siamese_network(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "cnn_siamese_network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "cnn_siamese_network.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZkd7N6i0aU",
        "outputId": "c0a0b4c4-a513-478d-f15c-f95dda80d4e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_19 (InputLayer)       [(None, 200, 6)]             0         []                            \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)       [(None, 200, 6)]             0         []                            \n",
            "                                                                                                  \n",
            " sequential_13 (Sequential)  (None, 256)                  785440    ['input_19[0][0]',            \n",
            "                                                                     'input_20[0][0]']            \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)           (None, 256)                  0         ['sequential_13[0][0]',       \n",
            "                                                                     'sequential_13[1][0]']       \n",
            "                                                                                                  \n",
            " dense_26 (Dense)            (None, 1)                    257       ['lambda_9[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 785697 (3.00 MB)\n",
            "Trainable params: 785697 (3.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the cnn model.\n",
        "cnn_siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # Training pairs\n",
        "    train_labels,  # Training labels\n",
        "    epochs=30,  # Number of epochs\n",
        "    batch_size=32,  # Batch size\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_labels)  # Validation data\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF2Lh1zL2DbL",
        "outputId": "7b3bdb4c-f658-4460-8368-56a5d1b7099a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.8289 - accuracy: 0.6154 - val_loss: 0.5850 - val_accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.5642 - accuracy: 0.5385 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.2925 - accuracy: 1.0000 - val_loss: 0.4260 - val_accuracy: 0.7500\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 1.0157 - accuracy: 0.4615 - val_loss: 0.3065 - val_accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.3346 - accuracy: 0.8462 - val_loss: 0.2362 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1342 - accuracy: 1.0000 - val_loss: 0.1910 - val_accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.3893 - accuracy: 0.8462 - val_loss: 0.1380 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.2241 - accuracy: 0.8462 - val_loss: 0.1205 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0943 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.1478 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.1021 - accuracy: 0.9231 - val_loss: 0.1490 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.1429 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.1352 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.1229 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1007 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0917 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0890 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0972 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1190 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 6.4958e-04 - accuracy: 1.0000 - val_loss: 0.1621 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 6.7513e-04 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.9993e-04 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 3.4860e-04 - accuracy: 1.0000 - val_loss: 0.2141 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 1.7063e-04 - accuracy: 1.0000 - val_loss: 0.2283 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.5633e-04 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 4.3048e-04 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a4da0f162c0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a HDF5 file (including architecture, weights, and training configuration)\n",
        "siamese_network.save('siamese_model.tflite')"
      ],
      "metadata": {
        "id": "6z9LZEGoDFbU"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}